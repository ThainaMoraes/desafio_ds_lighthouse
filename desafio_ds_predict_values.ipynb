{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio DS - Lighthouse - P02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding how to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Describing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "\n",
    "df_original = pd.read_csv(\"df_final_with_no_na_values.csv\",index_col='Year', parse_dates=True)\n",
    "df_original.index.name = None\n",
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.iloc[:-5,:].describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating a random list\n",
    "I decided to create a random list to better see what is happing with the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# random_columns_list = [random.randint(0,227) for _ in range(10)]\n",
    "# print(random_columns_list)\n",
    "# OUTPUT\n",
    "random_list = [1, 8, 23, 33, 55, 86, 92, 100, 150, 214]\n",
    "random_list_columns = df_original.iloc[:1,random_list].columns\n",
    "random_list_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Plot with seasonal decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_seasonal_decompose(df,list_columns):\n",
    "\n",
    "    for column in list_columns:\n",
    "        resultado = seasonal_decompose(df[column], period=10)\n",
    "        fig = plt.figure(figsize=(8, 6))  \n",
    "        fig = resultado.plot()\n",
    "\n",
    "test_seasonal_decompose(df_original,random_list_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Creating functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred, list_name, model_name):\n",
    "\n",
    "    dicio = []\n",
    "    for i in list_name:\n",
    "\n",
    "        y_true_2 = y_true[[i]].values\n",
    "        y_pred_2 = y_pred[[i]].values\n",
    "\n",
    "\n",
    "        mae = round(np.mean(np.abs((y_true_2 - y_pred_2) / y_true_2)) * 100, 2)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_true_2, y_pred_2)), 2)\n",
    "        mape = round(mean_absolute_percentage_error(y_true_2, y_pred_2), 2)\n",
    "\n",
    "        dicio.append({'Country':i,'Model': model_name,'mae': mae, 'rmse': rmse, 'mape': mape})\n",
    "    \n",
    "    metrics = pd.DataFrame(dicio)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adfuller\n",
    "\n",
    "This function tests the Augmented Dickey-Fuller test using a sample of countries and creates two lists: one for countries with stationary time series and another for countries with non-stationary time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adfuller(df,sample_list):\n",
    "    list_stationary = []\n",
    "    list_non_stationary = []\n",
    "\n",
    "    print(\"Result of p-value to sample data \\n\")\n",
    "    \n",
    "    for column in sample_list:\n",
    "        result = adfuller(df_original[column].iloc[:-5])\n",
    "        print(column)\n",
    "        print(f'ADF: {result[0]}')\n",
    "        print(f'p-value: {result[1]}')\n",
    "        print('-'*50)\n",
    "    \n",
    "    for column in df.columns:\n",
    "        result = adfuller(df_original[column].iloc[:-5])\n",
    "        if result[1] <= 0.05:\n",
    "            list_stationary.append(column)\n",
    "        else:\n",
    "            list_non_stationary.append(column)\n",
    "    \n",
    "    stationary_total = len(list_stationary)\n",
    "    stationary_p = (stationary_total/len(df.columns)*100)\n",
    "\n",
    "    non_stationary_total = len(list_non_stationary)\n",
    "    non_stationary_p = (non_stationary_total/len(df.columns)*100)\n",
    "    \n",
    "    print(f\"\\nTotal of stationary is {stationary_total} and it represents {stationary_p} %\")\n",
    "    print(f\"Total of Non stationary is {non_stationary_total} and it represents {non_stationary_p} %\")\n",
    "    \n",
    "    return (list_stationary,list_non_stationary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Auto ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_arima_model(df_train, list_columns, trend= None, d=None, stationary=True, seasonal=True):\n",
    "\n",
    "    df_prediction = pd.DataFrame()\n",
    "\n",
    "    for column in list_columns:\n",
    "        model_sarimax = auto_arima(\n",
    "            df_train[column],\n",
    "            start_p=0, max_p=2, \n",
    "            start_q=0, max_q=2,\n",
    "            # start_P=0, max_P=2,\n",
    "            # start_Q=0, max_Q=2,\n",
    "            m=10, \n",
    "            seasonal=seasonal,\n",
    "            stationary=stationary,\n",
    "            d=d, \n",
    "            trend= trend,\n",
    "            test = 'adf',\n",
    "            error_acCtion='ignore', \n",
    "            stepwise=True\n",
    "        )\n",
    "\n",
    "        pred_sarimax = model_sarimax.predict(df_test.shape[0])\n",
    "        \n",
    "        df_prediction = pd.concat([df_prediction, pd.DataFrame(pred_sarimax,columns=[column])], axis=1).round()\n",
    "\n",
    "    return pred_sarimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Simple Exponential Smoothing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Smoothing level\n",
    "\n",
    "This function finds the better smoothing level for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smoothing_level(df_train, df_test, columns):\n",
    "    smoothing_list = []\n",
    "\n",
    "    for i in columns:\n",
    "        \n",
    "        best_mse = float('inf')\n",
    "        best_smoothing_level = None\n",
    "        smoothing_levels = np.linspace(0.01, 1, 10)\n",
    "\n",
    "        for smoothing_level in smoothing_levels:\n",
    "            model = SimpleExpSmoothing(df_train[i]).fit(smoothing_level=smoothing_level)\n",
    "            forecast = model.forecast(steps=len(df_test[i]))\n",
    "            mse = mean_squared_error(df_test[i], forecast)\n",
    "            \n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_smoothing_level = smoothing_level\n",
    "\n",
    "        smoothing_list.append({'Country':i,'best_smoothing_level': best_smoothing_level})\n",
    "\n",
    "    smoothing_level = pd.DataFrame(smoothing_list)\n",
    "\n",
    "    return smoothing_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_smoothing(df_train, df_test, smoothing_level):\n",
    "\n",
    "    df_prediction = pd.DataFrame()\n",
    "    \n",
    "    for i in smoothing_level.index:\n",
    "        column = smoothing_level['Country'].iloc[i]\n",
    "        # print(column)\n",
    "        model_ses = SimpleExpSmoothing(\n",
    "            df_train[column],\n",
    "            initialization_method='estimated', #heurustic                             \n",
    "        ).fit(smoothing_level=smoothing_level['best_smoothing_level'].iloc[i], optimized=True)\n",
    "        \n",
    "        pred_ses = model_ses.forecast(df_test.shape[0])\n",
    "\n",
    "        df_prediction = pd.concat([df_prediction, pd.DataFrame(pred_ses,columns=[column])], axis=1).round()\n",
    "\n",
    "    simple_smoothing_pred = pd.DataFrame(df_prediction)\n",
    "    \n",
    "    return simple_smoothing_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Plot Test and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_pred(df_train,df_test,df_pred,random_columns_list, model_type):\n",
    "    columns = df_pred.iloc[:1,random_columns_list].columns\n",
    "    \n",
    "    for i in columns:\n",
    "        fig, axs = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "        axs.plot(df_train[i].index, df_train[i])\n",
    "        axs.plot(df_pred[i].index, df_pred[i])\n",
    "        axs.plot(df_test[i].index, df_test[i])\n",
    "        axs.legend(['Train','Pred','Test'])\n",
    "        axs.set_title(f'Model {model_type} - {i}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train series and Test series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_original.iloc[:-5].copy()\n",
    "df_test = df_original.iloc[44:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 List of stationary and no stationary series\n",
    "\n",
    "To find the list of countries that are stationary and no stationary and also testing de ADF with a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stationary,list_non_stationary = test_adfuller(df_train,random_list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_non_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Creating a random list with stationary series and non stationary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_stationary_list = [random.randint(0,193) for _ in range(10)]\n",
    "# random_non_stationary_list = [random.randint(0,33) for _ in range(10)]\n",
    "\n",
    "# print(random_stationary_list)\n",
    "# print(random_non_stationary_list)\n",
    "\n",
    "# OUTPUT\n",
    "random_stationary_list = [4, 15, 22, 58, 113, 131, 150, 181, 188, 189]\n",
    "random_non_stationary_list = [3, 4, 11, 11, 16, 24, 25, 29, 30, 33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Auto Arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Model Auto ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_auto_arima_stationary = auto_arima_model(df_train, list_stationary)\n",
    "df_pred_auto_arima_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Non Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_auto_arima_non_stationary = auto_arima_model(df_train, list_non_stationary, stationary = False, d=1)\n",
    "df_pred_auto_arima_non_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Metrics Auto ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_auto_arima_stationary = metrics(df_test, df_pred_auto_arima_stationary,list_stationary,'Auto ARIMA')\n",
    "df_metric_auto_arima_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Non Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_auto_arima_non_stationary = metrics(df_test, df_pred_auto_arima_non_stationary,list_non_stationary,'Auto ARIMA')\n",
    "df_metric_auto_arima_non_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Merge both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_auto_arima = pd.concat([df_metric_auto_arima_non_stationary,df_metric_auto_arima_stationary])\n",
    "df_metrics_auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Plots the prediction for some random series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_test_pred(df_train ,df_test,df_pred_auto_arima_stationary,random_stationary_list, 'Auto Arima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_pred(df_train ,df_test,df_pred_auto_arima_non_stationary,random_non_stationary_list,'SARIMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Simple Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Smoothing level\n",
    "Find smoothing level for each country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_level_stationary = find_smoothing_level(df_train, df_test, list_stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_level_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Non Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_level_non_stationary = find_smoothing_level(df_train, df_test, list_non_stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_level_non_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Model Simple Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_simple_smoothing_stationary =simple_smoothing(df_train, df_test, smoothing_level_stationary)\n",
    "df_pred_simple_smoothing_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Non Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_simple_smoothing_non_stationary =simple_smoothing(df_train, df_test, smoothing_level_non_stationary)\n",
    "df_pred_simple_smoothing_non_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 Metrics Simple Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_simple_smoothing_stationary = metrics(df_test, df_pred_simple_smoothing_stationary,list_stationary,'Simple Smoothing')\n",
    "df_metric_simple_smoothing_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) Non Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_simple_smoothing_non_stationary = metrics(df_test, df_pred_simple_smoothing_non_stationary,list_non_stationary,'Simple Smoothing')\n",
    "df_metric_simple_smoothing_non_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Merge both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_simple_smoothing = pd.concat([df_metric_simple_smoothing_non_stationary,df_metric_simple_smoothing_stationary])\n",
    "df_metrics_simple_smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4 Plots the prediction for some random series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_pred(df_train ,df_test,df_pred_simple_smoothing_stationary,random_stationary_list, 'Simple Smoothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_pred(df_train ,df_test,df_pred_simple_smoothing_non_stationary,random_non_stationary_list,'Simple Smoothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choosing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating a function to compare both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_calculate_percentages(df_a, df_b,column):\n",
    "    if len(df_a) != len(df_b):\n",
    "        raise ValueError(\"DataFrames must have the same number of rows.\")\n",
    "\n",
    "    total_rows = len(df_a)\n",
    "    count_a_greater = 0\n",
    "    count_b_greater = 0\n",
    "    count_equal = 0\n",
    "\n",
    "    for value_a, value_b in zip(df_a.iloc[:, column], df_b.iloc[:, column]):\n",
    "        # print(df_a.iloc[index:index+1,-2:-1])\n",
    "        if value_a > value_b:\n",
    "            count_a_greater += 1\n",
    "        elif value_a < value_b:\n",
    "            count_b_greater += 1\n",
    "        else:\n",
    "            count_equal += 1\n",
    "\n",
    "    percent_a_greater = (count_a_greater / total_rows) * 100\n",
    "    percent_b_greater = (count_b_greater / total_rows) * 100\n",
    "    percent_equal = (count_equal / total_rows) * 100\n",
    "\n",
    "    print(f\"Percentage of times A was greater than B: {percent_a_greater:.2f}%\")\n",
    "    print(f\"Percentage of times B was greater than A: {percent_b_greater:.2f}%\")\n",
    "    print(f\"Percentage of times A and B were equal: {percent_equal:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_calculate_percentages(df_metrics_auto_arima, df_metrics_simple_smoothing, column=-1)\n",
    "print('-'*60)\n",
    "compare_and_calculate_percentages(df_metrics_auto_arima, df_metrics_simple_smoothing, column=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final model\n",
    "\n",
    "Since simple exponential smoothing yielded better parameters compared to SARIMA, I have decided to use it as the final model for predicting the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Concat the stationary and non stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_model = pd.concat([df_pred_simple_smoothing_non_stationary,df_pred_simple_smoothing_stationary], axis = 1)\n",
    "final_pred_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Reorganizing the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column= final_pred_model.iloc[:, 0]\n",
    "\n",
    "remaining_columns= final_pred_model.iloc[:, 1:].reindex(sorted(final_pred_model.columns[1:]), axis=1)\n",
    "\n",
    "df_result_pred = pd.concat([first_column, remaining_columns], axis=1)\n",
    "\n",
    "df_result_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Merge train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_train,df_result_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Reindex and transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.reset_index()\n",
    "df_final.index = df_final.index.year\n",
    "\n",
    "df_final = df_final.reset_index()\n",
    "df_final.rename(columns ={'index':'Real GDP growth (Annual percent change)'}, inplace= True)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_transpose = df_final.set_index('Real GDP growth (Annual percent change)').transpose().reset_index(names=['Real GDP growth (Annual percent change)']).rename_axis('', axis=1).round(2)\n",
    "df_final_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_transpose.to_csv('files/predicted.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
